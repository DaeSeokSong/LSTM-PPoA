1. loss: 0.0000e+00 → choice about loss function have to again.

2. Approximate value loss: 1.0000 → Normalization is needed

3. When the graph about loss is parallel to the X-axis 
→ 3-1) last Dense layer's activation function delete or change another
→ 3-2) last Dense layer output just 1 result up to result count
→ 3-3) enough up to batch_size and learning rate

4. When the result about model's prediction is always same
→ 4-1) Increase the learning rate
→ 4-2) Input layer's activation delete
→ 4-3) Layer count down
→ 4-4) batch size down, epoch up (don't too much)
→ 4-5) optimizer change to another

5. If the loss function fluctuates a lot
→ 5-1) Check and change about Dropout %
→ 5-2) Check and change about Learning rate

6. When val_loss higher than loss (almost overfitting problem)
→ 6-1) Check and chage about Train:val Ratio (Ex. 8:2 -> 7:3)
→ 6-2) Check and chage about last Dense layer's activation function

7. When loss value repeat up and down
→ 7-1) Dropout layer's drop rate down
→ 7-1) Dropout layer delete