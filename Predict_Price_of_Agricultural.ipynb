{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Price of Agricultural.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwlqAUwZrbaSD5sH46oq7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/LSTM-PPoA/blob/main/Predict_Price_of_Agricultural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHp2QmQfL70"
      },
      "source": [
        "# 참조문헌\n",
        "## [A Prediction Model for Agricultural Products Price with LSTM Network](https://www.koreascience.or.kr/article/JAKO201809469053682.page)\n",
        "\n",
        "**신성호, 이미경, 송사광**\n",
        "\n",
        "한국과학기술정보연구원 연구데이터플랫폼센터,\n",
        "한국과학기술정보연구원 연구데이터플랫폼센터/과학기술연합대학원대학교 빅데이터과학과\n",
        "\n",
        "Sungho Shin(maximus74@kisti.re.kr), Mikyoung Lee(jerryis@kisti.re.kr),\n",
        "Sa-kwang Song(esmallj@kisti.re.kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EFNr9uiwNDj"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuQW7ij4HGQc"
      },
      "source": [
        "# 당년 농산물 가격 예측 LSTM 모델\n",
        "\n",
        "Input = 1~저번 달까지의 pram 값을 하나로 묶은 array(인스턴스)\n",
        "\n",
        "output = 당월의 해당 채소 가격\n",
        "\n",
        "layer = 원래 해당 채소 가격"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8AkO_e7n4kv"
      },
      "source": [
        "### Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imYIsQ-Qn6oB",
        "outputId": "bc029358-5b99-4e50-b2a9-ecf28f4b318a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/DeepLearning/Project/PPoA\n",
        "!ls -al"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/DeepLearning/Project/PPoA\n",
            "total 823\n",
            "drwx------ 2 root root   4096 Nov 24 06:07  Dataset\n",
            "-rw------- 1 root root 695680 Nov 29 12:14  PPoA_onion.h5\n",
            "-rw------- 1 root root 141891 Dec  6 11:53 'Predict Price of Agricultural.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urjpqmlxb99Q"
      },
      "source": [
        "### Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjLL_CiwGa2v"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import metrics, losses\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Bidirectional, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ER3CZzLmvJz"
      },
      "source": [
        "### Global variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slotXBAvmvRp"
      },
      "source": [
        "TARGET_YEAR = 2020\n",
        "START_YEAR = 2006\n",
        "END_YEAR = 2019\n",
        "CROPS = 'onion'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76aV88cLGCI-"
      },
      "source": [
        "### Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGHLnKkhrRt_"
      },
      "source": [
        "##### Data Read/Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6gFAHqPrR1G"
      },
      "source": [
        "def ReadDataset(start_year, end_year, last_prams, cur_prams) :\n",
        "    # 지난 최대 5년간 최대, 최소치를 기준으로 정규화해야 하는 항목들\n",
        "    df = pd.read_csv('./Dataset/'+CROPS+'/last_production_'+CROPS+'.csv', index_col=0, encoding='cp949')\n",
        "    # 전년 재배면적\n",
        "    last_cultiv_area = df['전년면적']\n",
        "    last_prams.append(last_cultiv_area)\n",
        "    # 전년 평균 생산단수\n",
        "    last_prod_unit = df['전년단수']\n",
        "    last_prams.append(last_prod_unit)\n",
        "    # 전년 평균 생산량(수확량)\n",
        "    last_production = df['전년생산량']\n",
        "    last_prams.append(last_production)\n",
        "\n",
        "    for next in range(0, (end_year-start_year)+1) :\n",
        "        cur_year = start_year + next\n",
        "\n",
        "        f_name = str(cur_year)+'_'+CROPS+'.csv'\n",
        "        df = pd.read_csv('./Dataset/'+CROPS+'/'+f_name, index_col=0)\n",
        "\n",
        "        ''' 기상변수 '''\n",
        "        # 강수량\n",
        "        precipi_avg = df['평균월강수량(mm)']\n",
        "        precipi_max = df['최다월강수량(mm)']\n",
        "        # 기온\n",
        "        temper_avg = df['평균기온(℃)']\n",
        "        temper_max = df['평균최고기온(℃)']\n",
        "        temper_min = df['평균최저기온(℃)']\n",
        "        # 풍속\n",
        "        windSpeed_avg = df['평균풍속(m/s)']\n",
        "        windSpeed_max = df['최대풍속(m/s)']\n",
        "        # 습도\n",
        "        humidity_avg = df['평균습도(%rh)']\n",
        "        humidity_min = df['최저습도(%rh)']\n",
        "        # 일조량 / 일사량\n",
        "        sunshine = df['일조합']\n",
        "        insolation = df['일사합']\n",
        "\n",
        "        ''' 기타변수 '''\n",
        "        # 전년 수입량\n",
        "        last_amount_import = df['전년수입량']\n",
        "        # 해당 농작물 가격 (index == 12)\n",
        "        crops_price = df['가격']\n",
        "        # 경유 가격\n",
        "        diesel_price = df['경유가격']\n",
        "        # 물가지수(price index -> pidx), 2015년 기준 얼마나 오르고 내렸는지\n",
        "        total_pidx = df['총물가지수']\n",
        "        prod_pidx = df['상품']\n",
        "        agricul_marine_prod_pidx = df['농축수산물']\n",
        "        indust_prod_pidx = df['공업제품']\n",
        "        serv_pidx = df['서비스']\n",
        "        pub_serv_pidx = df['공공서비스']\n",
        "        per_serv_pidx = df['개인서비스']\n",
        "        house_pidx = df['집세']\n",
        "\n",
        "        # 정규화 해야하는 데이터셋\n",
        "        prams = [precipi_avg, precipi_max,             # 강수량\n",
        "                temper_avg, temper_max, temper_min,    # 기온\n",
        "                windSpeed_avg, windSpeed_max,          # 풍속\n",
        "                humidity_avg, humidity_min,            # 습도\n",
        "                sunshine,                              # 일조량\n",
        "                insolation,                            # 일사량\n",
        "                crops_price,                           # 해당 농작물 월별 가격\n",
        "                diesel_price,                          # 월별 경유 가격\n",
        "                last_amount_import,                    # 전년 수입량\n",
        "                # 물가지수\n",
        "                total_pidx, prod_pidx, agricul_marine_prod_pidx, indust_prod_pidx, serv_pidx, pub_serv_pidx, per_serv_pidx, house_pidx,\n",
        "                ]\n",
        "\n",
        "        cur_prams.append(prams)\n",
        "\n",
        "def ArrangeDataset(X_dataset, y_dataset, norm_last, norm_cur) :\n",
        "    # X, y 데이터셋 구분\n",
        "    for cur, year in enumerate(norm_cur) :\n",
        "        tmp_X1 = [[], [], [], [], []] # 1~5월\n",
        "        tmp_y1 = [] # 6월\n",
        "        tmp_X2 = [[], [], [], [] ,[]] # 7~11월\n",
        "        tmp_y2 = [] # 12월\n",
        "        for idx, data in enumerate(year) :\n",
        "            for month in range(0, 12) :\n",
        "                if idx == 11 : \n",
        "                    if month == 5 : tmp_y1.append(data[month])\n",
        "                    elif month == 11: tmp_y2.append(data[month])\n",
        "                else :\n",
        "                    if month < 5 : tmp_X1[month].append(data[month])\n",
        "                    elif month > 5 and month < 11 : tmp_X2[month-6].append(data[month])\n",
        "\n",
        "        for last_data in norm_last :\n",
        "            for month in tmp_X1 : month.append(last_data[cur])\n",
        "            for month in tmp_X2 : month.append(last_data[cur])\n",
        "        \n",
        "        X_dataset.append(tmp_X1)\n",
        "        y_dataset.append(tmp_y1)\n",
        "        X_dataset.append(tmp_X2)\n",
        "        y_dataset.append(tmp_y2)\n",
        "\n",
        "    # 데이터 형변환\n",
        "    X_dataset = np.array(X_dataset)\n",
        "    y_dataset = np.array(y_dataset)\n",
        "\n",
        "    # reshape X_dataset\n",
        "    X_dataset = X_dataset.reshape(X_dataset.shape[0], X_dataset.shape[1] * X_dataset.shape[2], 1)\n",
        "\n",
        "    return X_dataset, y_dataset"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr51CZkZrNmk"
      },
      "source": [
        "##### Nomalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v5fm5afGCRQ"
      },
      "source": [
        "# 정규화\n",
        "def Normalization(targetData) :\n",
        "    return (targetData - targetData.min()) / (targetData.max() - targetData.min() + 1e-7) # 1e-7은 0으로 나누는 오류 예방차원\n",
        "\n",
        "def Normalize(last_prams, cur_prams) :\n",
        "    # last_prams\n",
        "    norm_last = []\n",
        "    for idx, pram in enumerate(last_prams) :\n",
        "        tmp_last = []\n",
        "        for next in range(0, (END_YEAR-START_YEAR)+1) :\n",
        "            tmp_df = pram\n",
        "\n",
        "            cur_year = START_YEAR + next\n",
        "            year_list = tmp_df.index.values.tolist()\n",
        "            if cur_year-4 < year_list[-1] : year_list = year_list[year_list.index(cur_year):len(year_list)] # 5년간 데이터가 없을 경우\n",
        "            else : year_list = year_list[year_list.index(cur_year):year_list.index(cur_year-4)+1]           # 5년간 데이터가 있는 경우\n",
        "\n",
        "            tmp_df = tmp_df.loc[year_list]\n",
        "            tmp_last.append(Normalization(tmp_df).to_numpy()[0])\n",
        "        norm_last.append(tmp_last)\n",
        "\n",
        "    # prams\n",
        "    for idx, data in enumerate(cur_prams) :\n",
        "        for i, pram in enumerate(data) :\n",
        "            pram = Normalization(pram)\n",
        "            pram = pram.to_numpy()\n",
        "\n",
        "            cur_prams[idx][i] = pram\n",
        "\n",
        "    return norm_last, cur_prams\n",
        "\n",
        "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
        "def Denormalization_min(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
        "def Denormalization_max(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.max()\n",
        "def Denormalization_avg(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + ((org_x_np.min() + org_x_np.max()) / 2)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbsM_WkpdNAn"
      },
      "source": [
        "### Data read"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li41MIACdNGY"
      },
      "source": [
        "# Read train dataset\n",
        "cur_prams = []\n",
        "last_prams = []\n",
        "ReadDataset(START_YEAR, END_YEAR, last_prams, cur_prams)\n",
        "\n",
        "# Read predicted dataset\n",
        "pred_cur_prams = []\n",
        "pred_last_prams = []\n",
        "ReadDataset(TARGET_YEAR, TARGET_YEAR, pred_last_prams, pred_cur_prams)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtnE8nF3927h"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR4aVA0E97aI"
      },
      "source": [
        "# Arrange train dataset\n",
        "X_dataset = []\n",
        "y_dataset = []\n",
        "last_prams, cur_prams = Normalize(last_prams, cur_prams)\n",
        "X_dataset, y_dataset = ArrangeDataset(X_dataset, y_dataset, last_prams, cur_prams)\n",
        "\n",
        "# Arrange predicted dataset\n",
        "pred_X_dataset = []\n",
        "pred_y_dataset = []\n",
        "pred_last_prams, pred_cur_prams = Normalize(pred_last_prams, pred_cur_prams)\n",
        "pred_X_dataset, pred_y_dataset = ArrangeDataset(pred_X_dataset, pred_y_dataset, pred_last_prams, pred_cur_prams)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p-DWnxJsVXf"
      },
      "source": [
        "### Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyig-hprrz8E",
        "outputId": "b8febb99-fc61-4d3c-d298-9d2689d85efd"
      },
      "source": [
        "# Data division (Train : Test = 9 : 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, test_size = 0.1)\n",
        "print(\"########## Train + Test (X,) (y,) / Test (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# Data division (Train : Validation = 7 : 3)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3)\n",
        "print(\"########## Train (X,) (y,) / Validation (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########## Train + Test (X,) (y,) / Test (X,) (y,) ##########\n",
            "(25, 120, 1) (25, 1) (3, 120, 1) (3, 1)\n",
            "########## Train (X,) (y,) / Validation (X,) (y,) ##########\n",
            "(17, 120, 1) (17, 1) (8, 120, 1) (8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P4ubOMj-YWG"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GpM_MnL-Ydz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e0fe6b-bff7-46c4-d659-e87c4f85ec85"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(64,                        # 해당 층의 노드 개수\n",
        "                       input_shape=(17, 120, 1),        # input_shape=?\n",
        "                       return_sequences=True)))         # return_sequences == 각 시퀀스를 출력할지\n",
        "#model.add(Dropout(0.01))                               # 과적합 방지용 Ex. Dropout 20%(==0.2)\n",
        "model.add(Bidirectional(LSTM(16)))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "model.build(input_shape=(17, 120, 1))\n",
        "model.compile(loss=losses.MeanSquaredError(),\n",
        "              optimizer=Adam(learning_rate=0.000025),    # pram ex. learning_rate=0.0001\n",
        "              metrics=[metrics.MeanSquaredError()]\n",
        "              )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_36 (Bidirecti  (17, 120, 128)           33792     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_37 (Bidirecti  (17, 32)                 18560     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_18 (Dense)            (17, 1)                   33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,385\n",
            "Trainable params: 52,385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQKnA-yk-bs5"
      },
      "source": [
        "### Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pgorD0U-bLv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b7e45e9-5772-41b5-b40e-c2bad7d7bc04"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data = (X_val, y_val),\n",
        "                    batch_size = 24,                    # Train set's 1~2%, current = 1~2%\n",
        "                    epochs = 612,                       # Train set's 10%, current = 20%\n",
        "                    verbose = 1,                        # 0=silent, 1=progress bar, 2=one line per epoch.\n",
        "                    )\n",
        "\n",
        "# loss and acc graph (train nd val)\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[[\"loss\", \"val_loss\"]].plot()\n",
        "\n",
        "# Acc and Loss about real data\n",
        "learning_lost, learning_err = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(\"Learning error % :\", learning_err * 100)\n",
        "print(\"Learning loss % :\", learning_lost * 100)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/612\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.0942 - mean_squared_error: 0.0942 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
            "Epoch 2/612\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0936 - mean_squared_error: 0.0936 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
            "Epoch 3/612\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0930 - mean_squared_error: 0.0930 - val_loss: 0.2810 - val_mean_squared_error: 0.2810\n",
            "Epoch 4/612\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0924 - mean_squared_error: 0.0924 - val_loss: 0.2795 - val_mean_squared_error: 0.2795\n",
            "Epoch 5/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0919 - mean_squared_error: 0.0919 - val_loss: 0.2780 - val_mean_squared_error: 0.2780\n",
            "Epoch 6/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0913 - mean_squared_error: 0.0913 - val_loss: 0.2766 - val_mean_squared_error: 0.2766\n",
            "Epoch 7/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.2752 - val_mean_squared_error: 0.2752\n",
            "Epoch 8/612\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0902 - mean_squared_error: 0.0902 - val_loss: 0.2737 - val_mean_squared_error: 0.2737\n",
            "Epoch 9/612\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0897 - mean_squared_error: 0.0897 - val_loss: 0.2723 - val_mean_squared_error: 0.2723\n",
            "Epoch 10/612\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.2709 - val_mean_squared_error: 0.2709\n",
            "Epoch 11/612\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0886 - mean_squared_error: 0.0886 - val_loss: 0.2694 - val_mean_squared_error: 0.2694\n",
            "Epoch 12/612\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.2680 - val_mean_squared_error: 0.2680\n",
            "Epoch 13/612\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.0875 - mean_squared_error: 0.0875 - val_loss: 0.2666 - val_mean_squared_error: 0.2666\n",
            "Epoch 14/612\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0870 - mean_squared_error: 0.0870 - val_loss: 0.2652 - val_mean_squared_error: 0.2652\n",
            "Epoch 15/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0865 - mean_squared_error: 0.0865 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
            "Epoch 16/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0860 - mean_squared_error: 0.0860 - val_loss: 0.2625 - val_mean_squared_error: 0.2625\n",
            "Epoch 17/612\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0855 - mean_squared_error: 0.0855 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
            "Epoch 18/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0850 - mean_squared_error: 0.0850 - val_loss: 0.2597 - val_mean_squared_error: 0.2597\n",
            "Epoch 19/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0845 - mean_squared_error: 0.0845 - val_loss: 0.2584 - val_mean_squared_error: 0.2584\n",
            "Epoch 20/612\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0840 - mean_squared_error: 0.0840 - val_loss: 0.2570 - val_mean_squared_error: 0.2570\n",
            "Epoch 21/612\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.0836 - mean_squared_error: 0.0836 - val_loss: 0.2557 - val_mean_squared_error: 0.2557\n",
            "Epoch 22/612\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0831 - mean_squared_error: 0.0831 - val_loss: 0.2544 - val_mean_squared_error: 0.2544\n",
            "Epoch 23/612\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0826 - mean_squared_error: 0.0826 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
            "Epoch 24/612\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.0822 - mean_squared_error: 0.0822 - val_loss: 0.2518 - val_mean_squared_error: 0.2518\n",
            "Epoch 25/612\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.0817 - mean_squared_error: 0.0817 - val_loss: 0.2505 - val_mean_squared_error: 0.2505\n",
            "Epoch 26/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0813 - mean_squared_error: 0.0813 - val_loss: 0.2492 - val_mean_squared_error: 0.2492\n",
            "Epoch 27/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0809 - mean_squared_error: 0.0809 - val_loss: 0.2479 - val_mean_squared_error: 0.2479\n",
            "Epoch 28/612\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.2466 - val_mean_squared_error: 0.2466\n",
            "Epoch 29/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0800 - mean_squared_error: 0.0800 - val_loss: 0.2454 - val_mean_squared_error: 0.2454\n",
            "Epoch 30/612\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.2441 - val_mean_squared_error: 0.2441\n",
            "Epoch 31/612\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0792 - mean_squared_error: 0.0792 - val_loss: 0.2429 - val_mean_squared_error: 0.2429\n",
            "Epoch 32/612\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0788 - mean_squared_error: 0.0788 - val_loss: 0.2416 - val_mean_squared_error: 0.2416\n",
            "Epoch 33/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.2404 - val_mean_squared_error: 0.2404\n",
            "Epoch 34/612\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.0780 - mean_squared_error: 0.0780 - val_loss: 0.2392 - val_mean_squared_error: 0.2392\n",
            "Epoch 35/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0776 - mean_squared_error: 0.0776 - val_loss: 0.2380 - val_mean_squared_error: 0.2380\n",
            "Epoch 36/612\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.0772 - mean_squared_error: 0.0772 - val_loss: 0.2368 - val_mean_squared_error: 0.2368\n",
            "Epoch 37/612\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 0.0769 - mean_squared_error: 0.0769 - val_loss: 0.2356 - val_mean_squared_error: 0.2356\n",
            "Epoch 38/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0765 - mean_squared_error: 0.0765 - val_loss: 0.2345 - val_mean_squared_error: 0.2345\n",
            "Epoch 39/612\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.2333 - val_mean_squared_error: 0.2333\n",
            "Epoch 40/612\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.2321 - val_mean_squared_error: 0.2321\n",
            "Epoch 41/612\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.2310 - val_mean_squared_error: 0.2310\n",
            "Epoch 42/612\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0751 - mean_squared_error: 0.0751 - val_loss: 0.2299 - val_mean_squared_error: 0.2299\n",
            "Epoch 43/612\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0748 - mean_squared_error: 0.0748"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-8d3e5551dccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0;31m# Train set's 1~2%, current = 1~2%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m612\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# Train set's 10%, current = 20%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m                        \u001b[0;31m# 0=silent, 1=progress bar, 2=one line per epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1261\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1264\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5kJKvINlQ0H"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS7FBclZlQ8a"
      },
      "source": [
        "#model = load_model('PPoA_onion.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmw7-DI7-f2S"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJrzhLK1-hLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4955e2c-08f0-43f2-92c0-9f9162b63bcc"
      },
      "source": [
        "pred_res = model.predict(pred_X_dataset)\n",
        "\n",
        "pred_min_price = Denormalization_min(ori_prices, pred_res)\n",
        "for idx, price in enumerate(pred_min_price) : pred_min_price[idx] = round(price[0])\n",
        "pred_max_price = Denormalization_max(ori_prices, pred_res)\n",
        "for idx, price in enumerate(pred_max_price) : pred_max_price[idx] = round(price[0])\n",
        "pred_avg_price = Denormalization_avg(ori_prices, pred_res)\n",
        "for idx, price in enumerate(pred_avg_price) : pred_avg_price[idx] = round(price[0])\n",
        "    \n",
        "price_June = {}\n",
        "price_Dec = {}\n",
        "for idx in range(0, 2) :\n",
        "    min = pred_min_price[idx].astype(int)[0]\n",
        "    avg = pred_avg_price[idx].astype(int)[0]\n",
        "    max = pred_max_price[idx].astype(int)[0]\n",
        "    if idx == 0 :\n",
        "        price_June['최저가'] = min\n",
        "        price_June['평균가'] = avg\n",
        "        price_June['최고가'] = max\n",
        "    else :\n",
        "        price_Dec['최저가'] = min\n",
        "        price_Dec['평균가'] = avg\n",
        "        price_Dec['최고가'] = max\n",
        "\n",
        "for scale, price in price_June.items() :\n",
        "    print(\"6월달 예상 \"+ scale +\" = \", price)\n",
        "print('----------------------------')\n",
        "for scale, price in price_Dec.items() :\n",
        "    print(\"12월달 예상 \"+ scale +\" = \", price)\n",
        "print('----------------------------')\n",
        "for idx, price in enumerate(pred_y_dataset.tolist()) :\n",
        "    if idx == 0 : print(\"실제 6월 양파 가격 = \", price[0])\n",
        "    else : print(\"실제 12월 양파 가격 = \", price[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6월달 예상 최저가 =  686\n",
            "6월달 예상 평균가 =  1014\n",
            "6월달 예상 최고가 =  1342\n",
            "----------------------------\n",
            "12월달 예상 최저가 =  881\n",
            "12월달 예상 평균가 =  1209\n",
            "12월달 예상 최고가 =  1537\n",
            "----------------------------\n",
            "실제 6월 양파 가격 =  569\n",
            "실제 12월 양파 가격 =  1225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjZvuFAlF320"
      },
      "source": [
        "### Model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsu6tf6yF5qY"
      },
      "source": [
        "#model.save('PPoA_onion.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}